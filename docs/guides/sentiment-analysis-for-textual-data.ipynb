{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de3a328-199b-49e0-b996-4ce86a4654f1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for textual data\n",
    "\n",
    "Data analysis often starts with **structured data** that’s **** already stored as numbers, dates, categories etc. However, unstructured data can yield crucial insights if you use appropriate techniques. Often you need to create a report for your CS team using NLP... In this tutorial, we'll run sentiment analysis on a textual dataset to figure out how positive and negative each phrase is, and turn the results into an interactive Datapane report.&#x20;\n",
    "\n",
    "## The dataset\n",
    "\n",
    "Let’s imagine we’re a data scientist working for a news company and we’re trying to figure out how ‘positive’ our news headlines are in comparison to the industry.\n",
    "\n",
    "We’ll start with the [UCI News Aggregator dataset](https://www.kaggle.com/uciml/news-aggregator-dataset) which is a collection of news headlines from different publications in 2014. This is a fun dataset because it has articles from a wide range of publishers and contains useful metadata.&#x20;\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"~/uci-news-aggregator.csv\")\n",
    "\n",
    "# Convert UNIX timestamps in milliseconds since 1970 into datetimes\n",
    "raw_data[\"TIMESTAMP\"] = pd.to_datetime(raw_data[\"TIMESTAMP\"], unit = 'ms')\n",
    "\n",
    "# Add more informative category names\n",
    "di = {\"b\": \"business\",\n",
    "      \"t\" : \"science and technology\",\n",
    "      \"e\" : \"entertainment\",\n",
    "      \"m\" : \"health\"}\n",
    "raw_data.replace({\"CATEGORY\": di}, inplace=True)\n",
    "\n",
    "raw_data.info()\n",
    "raw_data.head()\n",
    "```\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1\\*6xK9VG0egNnCx\\_WERYyGdw.png)\n",
    "\n",
    "We have 8 columns and about 400k rows. We’ll use the ‘Title’ for the actual sentiment analysis, and group the results by ‘Publisher’, ‘Category’ and ‘Timestamp’.&#x20;\n",
    "\n",
    "### **Classifying the headlines**\n",
    "\n",
    "Through the magic of open-source, we can use someone else’s hard-earned knowledge in our analysis — in this case a pretrained model called the Vader Sentiment Intensity Analyser from the popular [NLTK](https://www.nltk.org/index.html) library.\n",
    "\n",
    "To build the model, the authors gathered a list of common words and then asked a panel of human testers to rate each one on **valence** i.e. positive or negative, and **intensity** i.e. how strong the sentiment is. As the [original paper](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf) says: :&#x20;\n",
    "\n",
    "> \\[After stripping out irrelevant words] this left us with just over 7,500 lexical features with validated valence scores that indicated both the sentiment polarity (positive/negative), and the sentiment intensity on a scale from –4 to +4. For example, the word “okay” has a positive valence of 0.9, “good” is 1.9, and “great” is 3.1, whereas “horrible” is –2.5, the frowning emoticon “:(” is –2.2, and “sucks” and “sux” are both –1.5.\n",
    "\n",
    "To classify a piece of text, the model calculates the valence score for each word, applies some grammatical rules e.g. distinguishing between ‘great’ and ‘not great’, and then sums up the result.\n",
    "\n",
    "Interestingly, this simple lexicon-based approach has equal or better accuracy compared to machine-learning approaches, and is much faster. Let’s see how it works!\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "\n",
    "results = [sia.polarity_scores(line) for line in raw_data.TITLE]\n",
    "scores_df = pd.DataFrame.from_records(results)\n",
    "df = scores_df.join(raw_data, rsuffix=\"_right\")\n",
    "\n",
    "df.head()\n",
    "```\n",
    "\n",
    "In this code we import the library, classify each title in our dataset then append the results to our original dataframe. We have added 4 new columns:&#x20;\n",
    "\n",
    "* **pos**: positive score component\n",
    "* **neu**: neutral score component\n",
    "* **neg**: negative score component\n",
    "* **compound**: the sum of the three score components\n",
    "\n",
    "As a sanity check, let’s take a look at the most positive, neutral and negative headline in the text by using pandas `idxmax` :&#x20;\n",
    "\n",
    "```python\n",
    "negative = df.iloc[df.neg.idxmax()]\n",
    "neutral = df.iloc[df.neu.idxmax()]\n",
    "positive = df.iloc[df.pos.idxmax()]\n",
    "print(f'Most negative: {negative.TITLE} ({negative.PUBLISHER})')\n",
    "print(f'Most neutral: {neutral.TITLE} ({neutral.PUBLISHER})')\n",
    "print(f'Most positive: {positive.TITLE} ({positive.PUBLISHER})')\n",
    "```\n",
    "\n",
    "Running that code gives us the following result:&#x20;\n",
    "\n",
    "```\n",
    "Most negative: I hate cancer (Las Vegas Review-Journal \\(blog\\))\n",
    "Most neutral: Fed's Charles Plosser sees high bar for change in pace of tapering (Livemint)\n",
    "Most positive: THANK HEAVENS (Daily Beast)\n",
    "```\n",
    "\n",
    "Fair enough — ‘THANKS HEAVENS’ is a lot more positive than ‘I hate cancer’!\n",
    "\n",
    "## Visualizing the results\n",
    "\n",
    "When we're building our report, we need great visuals...\n",
    "\n",
    "What does the distribution of our scores look like? Let’s visualize this in a couple of ways using the interactive plotting library [Altair](https://altair-viz.github.io/index.html):&#x20;\n",
    "\n",
    "```python\n",
    "import altair as alt\n",
    "\n",
    "df[\"compound_trunc\"] = df.compound.round(1) # Truncate compound scores into 0.1 buckets \n",
    "\n",
    "res = (df.groupby([\"compound_trunc\",\"CATEGORY\"])[\"ID\"]\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"ID\": \"count\"})\n",
    "      )\n",
    "\n",
    "hist = alt.Chart(res).mark_bar(width=15).encode(\n",
    "    alt.X(\"compound_trunc:Q\", axis=alt.Axis(title=\"\")),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title=\"\")),\n",
    "    color=alt.Color('compound_trunc:Q', scale=alt.Scale(scheme='redyellowgreen')), \n",
    "    tooltip=['compound_trunc', 'count']\n",
    ")\n",
    "\n",
    "stacked_bar = alt.Chart(res).mark_bar().encode(\n",
    "    x = \"CATEGORY\",\n",
    "    y=alt.Y('count:Q', stack='normalize', axis=alt.Axis(title=\"\", labels=False)),\n",
    "    color=alt.Color('compound_trunc', scale=alt.Scale(scheme='redyellowgreen')), \n",
    "    tooltip=['compound_trunc', 'CATEGORY', 'count'],\n",
    "    order=alt.Order(\n",
    "      # Sort the segments of the bars by this field\n",
    "      'compound_trunc',\n",
    "      sort='ascending')\n",
    ").properties(width = 150)\n",
    "\n",
    "hist\n",
    "```\n",
    "\n",
    "Here we’re showing both a histogram for the overall distribution, as well as a 100% stacked bar chart grouped by category. Running that code, we get the following result:&#x20;\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1\\*cM8fnztmpLYn4DHGMGtluA.png)\n",
    "\n",
    "Seems like most headlines are neutral, and health has overall more negative articles than the other categories.&#x20;\n",
    "\n",
    "To give more insight into how our model is classifying the articles, we can create two more plots, one showing a sample of how the model classifies particular headlines, and another showing the average sentiment score for our largest publishers over time:&#x20;\n",
    "\n",
    "```python\n",
    "# Plot a random sample of 5k articles\n",
    "scatter = alt.Chart(df.sample(n=5000, random_state=1)).mark_point().encode(\n",
    "    alt.X(\"TIMESTAMP\", axis=alt.Axis(title=\"\")),\n",
    "    y=alt.Y('compound', axis=alt.Axis(title=\"\")),\n",
    "    color=alt.Color('compound:Q', scale=alt.Scale(scheme='redyellowgreen')), \n",
    "    tooltip=['TITLE', 'PUBLISHER','compound:Q', 'TIMESTAMP']\n",
    ")\n",
    "\n",
    "# Get the 10 largest publishers\n",
    "largest_10 = (df.groupby(by=[\"PUBLISHER\"])[\"ID\"]\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"ID\": \"count\"})\n",
    "        .nlargest(10, 'count')\n",
    "      )\n",
    "\n",
    "# Truncate by 30-day periods\n",
    "df[\"date\"] = df['TIMESTAMP'].dt.floor(freq='30D')\n",
    "\n",
    "line = alt.Chart(df[df.PUBLISHER.isin(largest_10.PUBLISHER)]).mark_line(clip=True).encode(\n",
    "    alt.X(\"date\", axis=alt.Axis(title=\"\")),\n",
    "    y=alt.Y('average(compound)', axis=alt.Axis(title=\"\"), scale=alt.Scale(domain=(-0.15, 0.15))),\n",
    "    color=alt.Color('PUBLISHER:O'), \n",
    "    tooltip=['PUBLISHER','average(compound):Q', 'date']\n",
    ")\n",
    "\n",
    "line\n",
    "```\n",
    "\n",
    "This is where Altair really shines - its declarative syntax means you can change just one or two keywords to get an entirely different view on the data. Running that code gives us the following result:&#x20;\n",
    "\n",
    "{% embed url=\"https://datapane.com/u/johnmicahreid/reports/0keR9XA/scatter-and-line-plots/embed\" %}\n",
    "\n",
    "By creating interactive visualizations, you enable viewers to explore the data directly. They’ll be much more likely to trust your overall conclusions if they can drill down to the original datapoints.&#x20;\n",
    "\n",
    "Looking at the publishers chart it seems that HuffPost is consistently more negative and RTT more positive. Hmmm, seems like they have different editorial policies…\n",
    "\n",
    "## Creating a Datapane report\n",
    "\n",
    "The final step is to package the results into an interactive Datapane report so that others can interact with and understand the data.&#x20;\n",
    "\n",
    "After logging into our Datapane account, we'll wrap our plots inside `dp.Plot` blocks, add some additional pages and written context:&#x20;\n",
    "\n",
    "```python\n",
    "import datapane as dp\n",
    "\n",
    "# dp.login(token=\"yourtoken\")\n",
    "\n",
    "dp.Report(\n",
    "    dp.Page(\n",
    "        dp.Text(\n",
    "\"\"\"\n",
    "# Sentiment Analysis of News Headlines\n",
    "This report uses a sentiment analysis model to determine the positivity/negativity of news headlines from the [UCI News Dataset](https://www.kaggle.com/uciml/news-aggregator-dataset).         \n",
    "\"\"\"\n",
    "      ),\n",
    "        dp.Group(\n",
    "            dp.Plot(hist), \n",
    "            dp.Plot(stacked_bar),\n",
    "            columns=2\n",
    "        ),\n",
    "        dp.Text(\"\"\"\n",
    "Scores are unimodal, with over 50% of headlines classified as 'neutral'. Health appears to be the most negative news category. \n",
    "\n",
    "## Examples and publishers\n",
    "\n",
    "To explore individual headlines, hover over the individual scatter points below: \n",
    "        \"\"\"\n",
    "               ),\n",
    "        dp.Plot(scatter),\n",
    "        dp.Plot(line, label = \"Top 10 publishers average monthly sentiment\"),\n",
    "        dp.Text(\"\"\"\n",
    "Of our top 10 publishers, it looks like Huffpost is most consistently negative, and RTT Today most positive.\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "....\n",
    "        \n",
    "        \"\"\"),\n",
    "        title = \"Charts\"\n",
    "    ), dp.Page(\n",
    "        dp.DataTable(df),\n",
    "        title = \"Selected Data\"\n",
    "    )\n",
    ").upload(name=\"Distribution_of_Sentiment\")\n",
    "```\n",
    "\n",
    "Running that code gives us the following:&#x20;\n",
    "\n",
    "{% embed url=\"https://datapane.com/u/johnmicahreid/reports/Okp4JXk/distribution-of-sentiment/embed\" %}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
